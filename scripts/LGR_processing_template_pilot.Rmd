---
title: "LGR_processing_template_pilot"
author: "abbey yatsko"
date: "8/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# LGR Processing Template - PILOT

## PART 1 - INTRODUCTION ----
### The purpose of this document is to outline the necessary steps to process and visualize gas flux data from the Los Gatos Research Ultraportable Greenhouse Gas Analyzer (LGR UGGA). Gas flux data comes from pieces of deadwood at different stages in the decay trajectory. This document uses pilot data collected from 16 samples of deadwood at the Smithsonian Environmental Research Center (SERC) in Edgewater, MD. All code is adapted from Genevieve Noyce (SERC biogeochemist).  

### Set workspace, load in raw data (.txt files taken directly from the LGR include necessary packages and call on libraries

#### for pilot data - .txt files are coming from two different days ('d1' and 'd2'), therefore two files need to be brought in from respective folders and formatted to be read as .csv

##### we do not want to merge these day 1 and day 2 files however, as the date/time conflict that would occur since the data spans 2 days would be really messy. therefore everything will be preprocessed seperately 

## PART 2 - PREPROCESS ----

setting workspace: 
```{r workspace}
library(data.table)
library(scales)
library(ggplot2)
library(ggpubr)
library(sqldf)
library(dplyr)

# day 1 data load and prep 
# set working directory
setwd("/Users/abbeyyatsko/Desktop/repos/serc_deadwood/pilot_LGR/2021-08-03")
# get names of all LGR files with GHG concentration, a.k.a the data with the form ('...f####.txt')
filenames_d1 <- list.files(pattern='f0',full.names=T)
# read in LGR datafiles to list and go from .txt to .csv 
data_d1 <- lapply(filenames_d1,read.csv,skip=1)
# combine all files into single dataframe for day 1 
dat_all_d1 <- do.call(rbind,data_d1)

# day 2 data load and prep 
# repeat above steps 
setwd("/Users/abbeyyatsko/Desktop/repos/serc_deadwood/pilot_LGR/2021-08-04")
filenames_d2 <- list.files(pattern='f0',full.names=T)
data_d2 <- lapply(filenames_d2,read.csv,skip=1)
dat_all_d2 <- do.call(rbind,data_d2)
```

formatting dates and times: 
```{r date/time}
# day 1 data
# pull out date and time data
date_time <- strptime(dat_all_d1[,1],format='%m/%d/%Y %H:%M:%S')
# add year,month,day,JD,hour,min,sec columns to dataframe
Year <- as.numeric(format(date_time,'%Y'))
Month <- as.numeric(format(date_time,'%m'))
Day <- as.numeric(format(date_time,'%d'))
fDOY <- as.numeric(julian(date_time,'2021-01-01'))  #Change for year
Hour <- as.numeric(format(date_time,'%k'))
Min <- as.numeric(format(date_time,'%M'))
Sec <- as.numeric(format(date_time,'%S'))
dat_all_d1 <- cbind(date_time,Year,Month,Day,fDOY,Hour,Min,Sec,dat_all_d1[,-1])
# save LGR data as data.table
dat_all_d1 <- data.table(dat_all_d1)

# day 2 data
# process same as above
date_time <- strptime(dat_all_d2[,1],format='%m/%d/%Y %H:%M:%S')
Year <- as.numeric(format(date_time,'%Y'))
Month <- as.numeric(format(date_time,'%m'))
Day <- as.numeric(format(date_time,'%d'))
fDOY <- as.numeric(julian(date_time,'2021-01-01'))  #Change for year
Hour <- as.numeric(format(date_time,'%k'))
Min <- as.numeric(format(date_time,'%M'))
Sec <- as.numeric(format(date_time,'%S'))
dat_all_d2 <- cbind(date_time,Year,Month,Day,fDOY,Hour,Min,Sec,dat_all_d2[,-1])
dat_all_d2 <- data.table(dat_all_d2)
```

essentially now what we have is some better-formatted raw data that came straight from the LGR itself. A good checkpoint is to quickly plot this so that we can see (roughly) the same visual that we got for the [GHGs] when the LGR was running in real time. let's do that below: 

preliminary viz: 
```{r prelimplots}
# day 1 data 
# for co2
co2_d1 <- ggplot(dat_all_d1, aes(date_time, X.CO2.d_ppm)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Day 1 CO2")
# for ch4 
ch4_d1 <- ggplot(dat_all_d1, aes(date_time, X.CH4.d_ppm)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Day 1 CH4")

# day 2 data 
# for co2
co2_d2 <- ggplot(dat_all_d2, aes(date_time, X.CO2.d_ppm)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Day 2 CO2")
# for ch4 
ch4_d2 <- ggplot(dat_all_d2, aes(date_time, X.CH4.d_ppm)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Day 2 CH4")

# view all plots together (day 1 and day 2)
ggarrange(co2_d1, ch4_d1, co2_d2, ch4_d2)
```

all looks good - this is remniscent of what we saw when the data was actually being collected in real time. time to move on and parse out the [GHGs] for when samples were recorded. 

## PART 3 - MERGE WITH METADATA ----
here, the goal is to take information about each individual sample that was measured for GHG flux and merge it with the flux readout that we just preprocessed. ultimately, we want to say for a given sample, here are all of the timepoints for which GHGs were measured over. then we can extract this information and have the change in GHGs through time (5 min sampling period) for each piece of deadwood that we sampled!

read in and format metadata, focusing on fraction Day of Year (fDOY):
```{r format metadata}
# day 1 data 
# read in file with metadata
setwd("/Users/abbeyyatsko/Desktop/repos/serc_deadwood/pilot_LGR/2021-08-03")
mdd1 <- read.csv('plot_metadata_day1.csv')
as.Date(mdd1$Date, '%m/%d/%y')

## Pull out date and time data
mdd1_date_time_start <- strptime(paste(mdd1$Date,mdd1$Time_start),'%m/%d/%y %H:%M')

## Add 30 seconds to set start time to be within flux period 
mdd1_date_time_start <- mdd1_date_time_start+30

## Add 5 min (300 sec) - the above 30 sec interlude (total s = 270) to flux start 
# time to get flux end time
mdd1_date_time_end <- mdd1_date_time_start+270

## Add fDOY columns for start and endtime to dataframe
fDOY_start <- as.numeric(julian(mdd1_date_time_start,'2021-01-01'))  #Change for year
fDOY_end <- as.numeric(julian(mdd1_date_time_end,'2021-01-01'))  #Change for year
mdd1 <- cbind(fDOY_start,fDOY_end,mdd1)

# day 2 data 
# repeat all  steps accordingly 
setwd("/Users/abbeyyatsko/Desktop/repos/serc_deadwood/pilot_LGR/2021-08-04")
mdd2 <- read.csv('plot_metadata_day2.csv')
as.Date(mdd2$Date, '%m/%d/%y')
mdd2_date_time_start <- strptime(paste(mdd2$Date,mdd2$Time_start),'%m/%d/%y %H:%M')
mdd2_date_time_start <- mdd2_date_time_start+30
mdd2_date_time_end <- mdd2_date_time_start+270
fDOY_start <- as.numeric(julian(mdd2_date_time_start,'2021-01-01'))  #Change for year
fDOY_end <- as.numeric(julian(mdd2_date_time_end,'2021-01-01'))  #Change for year
mdd2 <- cbind(fDOY_start,fDOY_end,mdd2)
```

merge LGR files to plot with metadata - the ultimate goal is to clip out samples and remove 'background' LGR [GHG] measurements (i.e., empty chamber measurements)

```{r sample clip}
# need to rename co2 and ch4 columns because naming format including '.' messes up the sql code 
dat_all_d1 <- dat_all_d1 %>%
  rename(CH4 = X.CH4._ppm,
         CO2 = X.CO2._ppm)
dat_all_d2 <- dat_all_d2 %>%
  rename(CH4 = X.CH4._ppm,
         CO2 = X.CO2._ppm)

# complete merge for day 1 samples
d1query <- sqldf("select dat_all_d1.fDOY, dat_all_d1.CO2, dat_all_d1.CH4, mdd1.PIECETAG 
from dat_all_d1 LEFT JOIN mdd1 ON (dat_all_d1.fDOY BETWEEN mdd1.fDOY_start AND mdd1.fDOY_end)")

# complete merge for day 2 samples

d2query <- sqldf("select dat_all_d2.fDOY, dat_all_d2.CO2, dat_all_d2.CH4, mdd2.PIECETAG 
from dat_all_d2 LEFT JOIN mdd2 ON (dat_all_d2.fDOY BETWEEN mdd2.fDOY_start AND mdd2.fDOY_end)")

# NOW that things are in working order and paired up: parse through only the data recorded for ch4 where PIECETAG is recorded - this means that the data actually means something and is assigned to an actual sample

# use 'complete.cases'
d1query <- d1query[complete.cases(d1query), ]
d2query <- d2query[complete.cases(d2query), ]

# now visually check clipped bits of data 
# day 1 samples
par(mfrow=c(2,1),mar=c(4,4,1,1))
with(dat_all_d1,plot(fDOY,CO2,ylim=c(400,1300)))
with(d1query,plot(fDOY,CO2,ylim=c(400,1300)))
# ch4 
par(mfrow=c(2,1),mar=c(4,4,1,1))
with(dat_all_d1,plot(fDOY,CH4,ylim=c(1.95,2.4)))
with(d1query,plot(fDOY,CH4,ylim=c(1.95,2.4)))

# day 2 samples
# co2
par(mfrow=c(2,1),mar=c(4,4,1,1))
with(dat_all_d2,plot(fDOY,CO2,ylim=c(400,1200)))
with(d2query,plot(fDOY,CO2,ylim=c(400,1200)))
# ch4 
par(mfrow=c(2,1),mar=c(4,4,1,1))
with(dat_all_d2,plot(fDOY,CH4,ylim=c(1.95,2.4)))
with(d2query,plot(fDOY,CH4,ylim=c(1.95,2.4)))
```

since the data is now merged, we can export the .csv file as an intermediate and do some last minute edits/additions to the file that will come into play in later steps

Add 'flag' columns and export merged data to .csv file saved on server:
```{r export data}
d1query$Flag_CH4='Y'
d1query$Flag_CO2='Y'
d1query$CH4_notes=''
d1query$CO2_notes=''
d2query$Flag_CH4='Y'
d2query$Flag_CO2='Y'
d2query$CH4_notes=''
d2query$CO2_notes=''

# export merged file - day 1
write.csv(d1query,"/Users/abbeyyatsko/Desktop/repos/serc_deadwood/pilot_LGR/working_files/day1_merged.csv", row.names = FALSE)
# export merged file - day 2
write.csv(d2query,"/Users/abbeyyatsko/Desktop/repos/serc_deadwood/pilot_LGR/working_files/day2_merged.csv", row.names = FALSE)
```


```{r graph seperate samples}
# day 1 co2 (plot 1) and ch4 (plot 2)
plot1 <- ggplot(d1query, aes(fDOY, CO2)) + 
  geom_point() + 
  facet_wrap( ~ PIECETAG, ncol = 3, scales = "free") + 
  stat_smooth(method = "lm") +
  theme_classic()
plot2 <- ggplot(d1query, aes(fDOY, CH4)) + 
  geom_point() + 
  facet_wrap( ~ PIECETAG, ncol = 3, scales = "free") + 
  stat_smooth(method = "lm") +
  theme_classic()

ggarrange(plot1, plot2)

# day 2 co2 (plot 3) and ch4 (plot 4)
plot3 <- ggplot(d2query, aes(fDOY, CO2)) + 
  geom_point() + 
  facet_wrap( ~ PIECETAG, ncol = 3, scales = "free") + 
  stat_smooth(method = "lm") +
  theme_classic()
plot4 <- ggplot(d2query, aes(fDOY, CH4)) + 
  geom_point() + 
  facet_wrap( ~ PIECETAG, ncol = 3, scales = "free") + 
  stat_smooth(method = "lm") +
  theme_classic()

ggarrange(plot3, plot4)
```

## PART 4 - PROCESS ----
dis is where we do the thing such that the data is starting to be processed. from the exported 'intermediate' files, we are now able to create linear plots using (fDOY ~ CO2/CH4) and then further seperating out individual records by PIECETAG. 

read in .csv, remove flagged points, and save CH4 and CO2 data separately 
